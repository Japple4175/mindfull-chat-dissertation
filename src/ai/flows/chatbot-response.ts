// This is an autogenerated file from Firebase Studio.
'use server';

/**
 * @fileOverview A chatbot conversation AI agent.
 *
 * - getChatbotResponse - A function that handles the chatbot conversation process.
 * - ChatbotResponseInput - The input type for the getChatbotResponse function.
 * - ChatbotResponseOutput - The return type for the getChatbotResponse function.
 */

import {ai} from '@/ai/genkit';
import {z} from 'genkit';

const ChatbotResponseInputSchema = z.object({
  message: z.string().describe('The user message to respond to.'),
  conversationHistory: z.array(z.object({
    role: z.enum(['user', 'assistant']),
    content: z.string(),
  })).optional().describe('The conversation history between the user and the bot.')
});
export type ChatbotResponseInput = z.infer<typeof ChatbotResponseInputSchema>;

const ChatbotResponseOutputSchema = z.object({
  response: z.string().describe('The response from the chatbot.'),
});
export type ChatbotResponseOutput = z.infer<typeof ChatbotResponseOutputSchema>;

export async function getChatbotResponse(input: ChatbotResponseInput): Promise<ChatbotResponseOutput> {
  return chatbotResponseFlow(input);
}

const prompt = ai.definePrompt({
  name: 'chatbotResponsePrompt',
  input: {schema: ChatbotResponseInputSchema},
  output: {schema: ChatbotResponseOutputSchema},
  prompt: `You are a mental health chatbot designed to provide supportive responses to users. Be kind and understanding. Respond to the user message while taking into account the conversation history.

Conversation history:
{{#each conversationHistory}}
{{role}}: {{content}}
{{/each}}

User message: {{{message}}}`,
  config: {
    safetySettings: [
      {
        category: 'HARM_CATEGORY_HATE_SPEECH',
        threshold: 'BLOCK_ONLY_HIGH',
      },
      {
        category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
        threshold: 'BLOCK_NONE',
      },
      {
        category: 'HARM_CATEGORY_HARASSMENT',
        threshold: 'BLOCK_MEDIUM_AND_ABOVE',
      },
      {
        category: 'HARM_CATEGORY_SEXUALLY_EXPLICIT',
        threshold: 'BLOCK_LOW_AND_ABOVE',
      },
    ],
  },
});

const chatbotResponseFlow = ai.defineFlow(
  {
    name: 'chatbotResponseFlow',
    inputSchema: ChatbotResponseInputSchema,
    outputSchema: ChatbotResponseOutputSchema,
  },
  async (input): Promise<ChatbotResponseOutput> => {
    try {
      const genkitResponse = await prompt(input);
      
      if (!genkitResponse.output) {
        // Log the full response for debugging if available
        console.error('Chatbot Flow Error: No output from AI model. Full Genkit response:', JSON.stringify(genkitResponse, null, 2));
        // Inspect candidates if available, they might contain reasons for no output (e.g. safety blocks)
        if (genkitResponse.candidates && genkitResponse.candidates.length > 0) {
          genkitResponse.candidates.forEach((candidate, index) => {
            console.error(`Candidate ${index}: Finish Reason - ${candidate.finishReason}, Message - ${candidate.message?.content}`);
             if (candidate.finishReason === 'SAFETY' && candidate.message?.content) {
               const safetyRatings = candidate.message.content.find(part => part.custom?.safetyRatings)?.custom.safetyRatings;
               if (safetyRatings) {
                 console.error('Safety Ratings:', JSON.stringify(safetyRatings, null, 2));
               }
            }
          });
        }
        throw new Error('The AI model did not return a valid response. This could be due to safety filters or other model issues. Please check server logs.');
      }
      return genkitResponse.output;
    } catch (e: any) {
      console.error('Error in chatbotResponseFlow:', e);
      // Re-throw a new error to ensure it's propagated, or transform into a user-friendly error structure.
      throw new Error(`Failed to get chatbot response: ${e.message || 'An unexpected error occurred in the AI flow.'}`);
    }
  }
);
